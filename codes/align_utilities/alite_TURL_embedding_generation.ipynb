{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import time,json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import trange\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from data_loader.hybrid_data_loaders import *\n",
    "from data_loader.header_data_loaders import *\n",
    "from data_loader.CT_Wiki_data_loaders import *\n",
    "from data_loader.RE_data_loaders import *\n",
    "from data_loader.EL_data_loaders import *\n",
    "from model.configuration import TableConfig\n",
    "\n",
    "from model.model import HybridTableMaskedLM, HybridTableCER, TableHeaderRanking, HybridTableCT,HybridTableEL,HybridTableRE,BertRE\n",
    "from model.transformers.configuration_bert import BertConfig\n",
    "from model.transformers.tokenization_bert import BertTokenizer\n",
    "from model.transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
    "from utils.util import *\n",
    "from baselines.row_population.metric import average_precision,ndcg_at_k\n",
    "from baselines.cell_filling.cell_filling import *\n",
    "from model import metric\n",
    "\n",
    "from scipy.spatial.distance import cosine, euclidean, jaccard\n",
    "import nltk, string, re\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "import glob\n",
    "from collections import Counter\n",
    "from random import sample\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import silhouette_score\n",
    "import codecs\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'\n",
    "config_name = \"configs/table-base-config_v2.json\"\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda')\n",
    "# load entity vocab from entity_vocab.txt\n",
    "entity_vocab = load_entity_vocab(data_dir, ignore_bad_title=True, min_ent_count=2)\n",
    "entity_wikid2id = {entity_vocab[x]['wiki_id']:x for x in entity_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class, model_class, _ = (TableConfig, HybridTableMaskedLM, BertTokenizer)\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.output_attentions = True\n",
    "\n",
    "checkpoint = \"checkpoint/\"\n",
    "model = model_class(config, is_simple=True)\n",
    "# checkpoint = torch.load(os.path.join(checkpoint, 'pytorch_model.bin'))\n",
    "checkpoint = torch.load(os.path.join(checkpoint, 'pytorch_model.bin'), map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(data_dir,\"CF_test_data.json\"), 'r') as f:\n",
    "#     dev_data = json.load(f)\n",
    "dataset = WikiHybridTableDataset(data_dir,entity_vocab,max_cell=100, max_input_tok=350, max_input_ent=150, src=\"dev\", max_length = [50, 10, 10], force_new=False, tokenizer = None, mode=0)\n",
    "# CF = cell_filling(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of converting an arbitrary table to input\n",
    "# Here we show an example for cell filling task\n",
    "# The input entites are entities in the subject column, we append [ENT_MASK] and use its representation to match with the candidate entities\n",
    "def CF_build_input1(pgEnt, pgTitle, secTitle, caption, headers, core_entities, core_entities_text, entity_cand, config):\n",
    "#     print(pgEnt) \n",
    "#     print(pgTitle)\n",
    "#     print(secTitle)\n",
    "#     print(caption)\n",
    "#     print(headers)\n",
    "#     print(core_entities)\n",
    "#     print(core_entities_text)\n",
    "#     print(entity_cand)\n",
    "    tokenized_pgTitle = config.tokenizer.encode(pgTitle, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    tokenized_meta = tokenized_pgTitle+\\\n",
    "                    config.tokenizer.encode(secTitle, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    if caption != secTitle:\n",
    "        tokenized_meta += config.tokenizer.encode(caption, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    tokenized_headers = [config.tokenizer.encode(header, max_length=config.max_header_length, add_special_tokens=False) for header in headers]\n",
    "    input_tok = []\n",
    "    input_tok_pos = []\n",
    "    input_tok_type = []\n",
    "    tokenized_meta_length = len(tokenized_meta)\n",
    "    input_tok += tokenized_meta\n",
    "    input_tok_pos += list(range(tokenized_meta_length))\n",
    "    input_tok_type += [0]*tokenized_meta_length\n",
    "    header_span = []\n",
    "    for tokenized_header in tokenized_headers:\n",
    "        tokenized_header_length = len(tokenized_header)\n",
    "        header_span.append([len(input_tok), len(input_tok)+tokenized_header_length])\n",
    "        input_tok += tokenized_header\n",
    "        input_tok_pos += list(range(tokenized_header_length))\n",
    "        input_tok_type += [1]*tokenized_header_length\n",
    "    \n",
    "    input_ent = [config.entity_wikid2id[pgEnt] if pgEnt!=-1 else 0]\n",
    "    input_ent_text = [tokenized_pgTitle[:config.max_cell_length]]\n",
    "    input_ent_type = [2]\n",
    "    \n",
    "    # core entities in the subject column\n",
    "    input_ent += [config.entity_wikid2id[entity] for entity in core_entities]\n",
    "    input_ent_text += [config.tokenizer.encode(entity_text, max_length=config.max_cell_length, add_special_tokens=False) if len(entity_text)!=0 else [] for entity_text in core_entities_text]\n",
    "    input_ent_type += [3]*len(core_entities)\n",
    "\n",
    "    # append [ent_mask]\n",
    "    input_ent += [config.entity_wikid2id['[ENT_MASK]']]*len(core_entities)\n",
    "    input_ent_text += [[]]*len(core_entities)\n",
    "    input_ent_type += [4]*len(core_entities)\n",
    "\n",
    "    input_ent_cell_length = [len(x) if len(x)!=0 else 1 for x in input_ent_text]\n",
    "    max_cell_length = max(input_ent_cell_length)\n",
    "    input_ent_text_padded = np.zeros([len(input_ent_text), max_cell_length], dtype=int)\n",
    "    for i,x in enumerate(input_ent_text):\n",
    "        input_ent_text_padded[i, :len(x)] = x\n",
    "    assert len(input_ent) == 1+2*len(core_entities)\n",
    "\n",
    "    input_tok_mask = np.ones([1, len(input_tok), len(input_tok)+len(input_ent)], dtype=int)\n",
    "    for header_i in header_span:\n",
    "        input_tok_mask[0, header_i[0]:header_i[1], len(input_tok)+1+len(core_entities):] = 0\n",
    "    input_tok_mask[0, :, len(input_tok)+1+len(core_entities):] = 0\n",
    "\n",
    "    # build the mask for entities\n",
    "    input_ent_mask = np.ones([1, len(input_ent), len(input_tok)+len(input_ent)], dtype=int)\n",
    "    for header_i in header_span[1:]:\n",
    "        input_ent_mask[0, 1:1+len(core_entities), header_i[0]:header_span[1][1]] = 0\n",
    "        input_ent_mask[0, 1:1+len(core_entities), len(input_tok)+1+len(core_entities):] = np.eye(len(core_entities), dtype=int)\n",
    "    input_ent_mask[0, 1+len(core_entities):, header_span[0][0]:header_span[0][1]] = 0\n",
    "    input_ent_mask[0, 1+len(core_entities):, len(input_tok)+1:len(input_tok)+1+len(core_entities)] = np.eye(len(core_entities), dtype=int)\n",
    "    input_ent_mask[0, 1+len(core_entities):, len(input_tok)+1+len(core_entities):] = np.eye(len(core_entities), dtype=int)\n",
    "\n",
    "    input_tok_mask = torch.LongTensor(input_tok_mask)\n",
    "    input_ent_mask = torch.LongTensor(input_ent_mask)\n",
    "\n",
    "    input_tok = torch.LongTensor([input_tok])\n",
    "    input_tok_type = torch.LongTensor([input_tok_type])\n",
    "    input_tok_pos = torch.LongTensor([input_tok_pos])\n",
    "    \n",
    "    input_ent = torch.LongTensor([input_ent])\n",
    "    input_ent_text = torch.LongTensor([input_ent_text_padded])\n",
    "    input_ent_cell_length = torch.LongTensor([input_ent_cell_length])\n",
    "    input_ent_type = torch.LongTensor([input_ent_type])\n",
    "\n",
    "    input_ent_mask_type = torch.zeros_like(input_ent)\n",
    "    input_ent_mask_type[:,1+len(core_entities):] = config.entity_wikid2id['[ENT_MASK]']\n",
    "    \n",
    "    candidate_entity_set = [config.entity_wikid2id[entity] for entity in entity_cand]\n",
    "    candidate_entity_set = torch.LongTensor([candidate_entity_set])\n",
    "    \n",
    "\n",
    "    return input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "            input_ent, input_ent_text, input_ent_cell_length, input_ent_type, input_ent_mask_type, input_ent_mask, candidate_entity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "def process(dev_data):\n",
    "    for table_id,pgEnt,pgTitle,secTitle,caption, headers, data_sample in tqdm(dev_data):\n",
    "        h1,h2 = headers\n",
    "#         print(table_id)\n",
    "#         print(pgEnt)\n",
    "#         print(pgTitle)\n",
    "#         print(secTitle)\n",
    "#         print(caption)\n",
    "#         print((h1, h2))\n",
    "#         print(data_sample)\n",
    "        result = []\n",
    "        while len(data_sample)!=0:\n",
    "            core_entities = []\n",
    "            core_entities_text = []\n",
    "            target_entities = []\n",
    "            all_entity_cand = set()\n",
    "            entity_cand = []\n",
    "            for (core_e, core_e_text), target_e in data_sample[:100]:\n",
    "                assert target_e in entity_wikid2id\n",
    "                core_entities.append(core_e)\n",
    "                core_entities_text.append(core_e_text)\n",
    "                target_entities.append(target_e)\n",
    "                cands = CF.get_cand_row(core_e, h2)\n",
    "                cands = {key:value for key,value in cands.items() if key in entity_wikid2id}\n",
    "                entity_cand.append(cands)\n",
    "                all_entity_cand |= set(cands.keys()) \n",
    "            all_entity_cand = list(all_entity_cand)\n",
    "            input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "                input_ent, input_ent_text, input_ent_text_length, input_ent_type, input_ent_mask_type, input_ent_mask, \\\n",
    "                candidate_entity_set = CF_build_input(pgEnt, pgTitle, secTitle, caption, headers, core_entities, core_entities_text, all_entity_cand, dataset)\n",
    "            input_tok = input_tok.to(device)\n",
    "            input_tok_type = input_tok_type.to(device)\n",
    "            input_tok_pos = input_tok_pos.to(device)\n",
    "            input_tok_mask = input_tok_mask.to(device)\n",
    "            input_ent_text = input_ent_text.to(device)\n",
    "            input_ent_text_length = input_ent_text_length.to(device)\n",
    "            input_ent = input_ent.to(device)\n",
    "            input_ent_type = input_ent_type.to(device)\n",
    "            input_ent_mask_type = input_ent_mask_type.to(device)\n",
    "            input_ent_mask = input_ent_mask.to(device)\n",
    "            candidate_entity_set = candidate_entity_set.to(device)\n",
    "            with torch.no_grad():\n",
    "                tok_outputs, ent_outputs = model(input_tok, input_tok_type, input_tok_pos, input_tok_mask,\n",
    "                                input_ent_text, input_ent_text_length, input_ent_mask_type,\n",
    "                                input_ent, input_ent_type, input_ent_mask, candidate_entity_set)\n",
    "                num_sample = len(target_entities)\n",
    "                ent_prediction_scores = ent_outputs[0][0,num_sample+1:].tolist()\n",
    "            for i, target_e in enumerate(target_entities):\n",
    "                predictions = ent_prediction_scores[i]\n",
    "                if len(entity_cand[i]) == 0:\n",
    "                    result.append([target_e, entity_cand[i], [], []])\n",
    "                else:\n",
    "                    tmp_cand_scores = []\n",
    "                    for j, cand_e in enumerate(all_entity_cand):\n",
    "                        if cand_e in entity_cand[i]:\n",
    "                            tmp_cand_scores.append([cand_e, predictions[j]])\n",
    "                    sorted_cand_scores =  sorted(tmp_cand_scores, key=lambda z:z[1], reverse=True)\n",
    "                    sorted_cands = [z[0] for z in sorted_cand_scores]\n",
    "                    # use H2H as baseline\n",
    "                    base_sorted_cands = CF.rank_cand_h2h(h2, entity_cand[i])\n",
    "                    result.append([target_e, entity_cand[i], sorted_cands, base_sorted_cands])\n",
    "            data_sample = data_sample[100:]\n",
    "        results.append({\n",
    "            'pgTitle': pgTitle,\n",
    "            'secTitle': secTitle,\n",
    "            'caption': caption,\n",
    "            'headers': results,\n",
    "            'result': result\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables_represenation(dev_data):\n",
    "    table_representations = {}\n",
    "    for table_id,pgEnt,pgTitle,secTitle,caption, headers, data_sample in tqdm(dev_data):\n",
    "        while len(data_sample)!=0:\n",
    "            core_entities = []\n",
    "            core_entities_text = []\n",
    "            target_entities = []\n",
    "            all_entity_cand = set()\n",
    "            entity_cand = []\n",
    "            for (core_e, core_e_text), target_e in data_sample[:100]:\n",
    "                assert target_e in entity_wikid2id\n",
    "                core_entities.append(core_e)\n",
    "                core_entities_text.append(core_e_text)\n",
    "                target_entities.append(target_e)\n",
    "                for h in headers[1:]:\n",
    "                    cands = CF.get_cand_row(core_e, h)\n",
    "                    cands = {key:value for key,value in cands.items() if key in entity_wikid2id}\n",
    "                    entity_cand.append(cands)\n",
    "                all_entity_cand |= set(cands.keys()) \n",
    "            all_entity_cand = list(all_entity_cand)\n",
    "            input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "                input_ent, input_ent_text, input_ent_text_length, input_ent_type, input_ent_mask_type, input_ent_mask, \\\n",
    "                candidate_entity_set = CF_build_input(pgEnt, pgTitle, secTitle, caption, headers, core_entities, core_entities_text, all_entity_cand, dataset)\n",
    "            input_tok = input_tok.to(device)\n",
    "            input_tok_type = input_tok_type.to(device)\n",
    "            input_tok_pos = input_tok_pos.to(device)\n",
    "            input_tok_mask = input_tok_mask.to(device)\n",
    "            input_ent_text = input_ent_text.to(device)\n",
    "            input_ent_text_length = input_ent_text_length.to(device)\n",
    "            input_ent = input_ent.to(device)\n",
    "            input_ent_type = input_ent_type.to(device)\n",
    "            input_ent_mask_type = input_ent_mask_type.to(device)\n",
    "            input_ent_mask = input_ent_mask.to(device)\n",
    "            candidate_entity_set = candidate_entity_set.to(device)\n",
    "            with torch.no_grad():\n",
    "                tok_outputs, ent_outputs = model(input_tok, input_tok_type, input_tok_pos, input_tok_mask,\n",
    "                                input_ent_text, input_ent_text_length, input_ent_mask_type,\n",
    "                                input_ent, input_ent_type, input_ent_mask, candidate_entity_set)\n",
    "            print(table_id)\n",
    "            print(ent_outputs[1].shape)\n",
    "            print(ent_outputs[1][0].shape)\n",
    "            table_representations[table_id] = tok_outputs, ent_outputs\n",
    "            data_sample = data_sample[100:]\n",
    "    return table_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_tables(folder):\n",
    "    tables = {}\n",
    "    table_to_cluster = {}\n",
    "    for folder_in in glob.glob(folder):\n",
    "        for file in glob.glob(folder_in+'/*'):\n",
    "            table_name = os.path.basename(file)\n",
    "            print(table_name)\n",
    "            table_to_cluster[table_name] = os.path.basename(folder_in)\n",
    "            table_content = pd.read_csv(file, encoding='latin-1')\n",
    "            tables[table_name] = table_content\n",
    "    return tables, table_to_cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    punc_with_ = [s for s in string.punctuation if s != '_']\n",
    "    text = re.sub('[%s]' % punc_with_, '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    nopunc = clean_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
    "    combined_text = ' '.join(tokenized_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_entity = {}\n",
    "for e in entity_vocab:\n",
    "    wiki_title = text_preprocessing(entity_vocab[e]['wiki_title'])\n",
    "    wiki_id = entity_vocab[e]['wiki_id']\n",
    "    text_to_entity[wiki_title] = wiki_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# torch.cuda\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an epsilon over the embedding change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table_representations(folder):\n",
    "    all_files = glob.glob(folder + \"/*.json\")\n",
    "    table_representations = {}\n",
    "    for filename in all_files:\n",
    "        temp = json.load(open(filename))\n",
    "        table_representations = {**table_representations, **temp}\n",
    "    return table_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_represenation_data_lake_increment(tables, \n",
    "                                                  text_to_entity, \n",
    "                                                  folder, \n",
    "                                                  sampling_size = 20, \n",
    "                                                  sampling_method = 'random', \n",
    "                                                  save_only_centroid = True, \n",
    "                                                  batches = 10, \n",
    "                                                  epsilon = 0.01):\n",
    "    times = []\n",
    "    table_representations = {}\n",
    "    save_format = folder + 'table_representations_{}_{}.json'\n",
    "    i = 0\n",
    "    for table_id in tqdm(tables):\n",
    "        table_representations[table_id] = {}\n",
    "        table = tables[table_id]\n",
    "        for col in table.columns:\n",
    "            start_time = time.time()\n",
    "            col_as_table = pd.DataFrame(table[col])\n",
    "            pgEnt = '[PAD]'\n",
    "            pgTitle = ''\n",
    "            secTitle = ''\n",
    "            caption = ''\n",
    "            headers = list(col_as_table.columns)\n",
    "            core_entities = [] # This will be the subject column entities\n",
    "            core_entities_text = []\n",
    "            all_entity_cand = []\n",
    "            if len(col_as_table) < sampling_size:\n",
    "                sampling_size_update = len(col_as_table) - 1\n",
    "            else:\n",
    "                sampling_size_update = sampling_size\n",
    "            iters = 0\n",
    "#             print(i)\n",
    "            while len(col_as_table) > sampling_size_update:\n",
    "                iters += 1\n",
    "#                 if i == 69:\n",
    "#                     print(iters)\n",
    "                table_subset = col_as_table.head(sampling_size)\n",
    "                col_as_table = col_as_table.iloc[sampling_size:, :]\n",
    "                for index, row in table_subset.iterrows():\n",
    "                    for columnIndex, value in row.items():\n",
    "                        entity = text_preprocessing(str(value).replace(' ', '_'))\n",
    "                        if entity in text_to_entity:\n",
    "                            core_entities.append(text_to_entity[entity])\n",
    "                            core_entities_text.append(entity) \n",
    "                            all_entity_cand.append(text_to_entity[entity])\n",
    "                        else:\n",
    "                            sub_entities = entity.split('_')\n",
    "                            if sub_entities != None:\n",
    "                                for sub_entity in sub_entities:\n",
    "                                    if sub_entity in text_to_entity:\n",
    "                                        core_entities.append(text_to_entity[sub_entity])\n",
    "                                        core_entities_text.append(sub_entity) \n",
    "                                        all_entity_cand.append(text_to_entity[sub_entity])\n",
    "                all_entity_cand = list(set(all_entity_cand))\n",
    "                input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "                        input_ent, input_ent_text, input_ent_text_length, input_ent_type, input_ent_mask_type, input_ent_mask, \\\n",
    "                        candidate_entity_set = CF_build_input1(pgEnt, pgTitle, secTitle, caption, headers, core_entities, core_entities_text, all_entity_cand, dataset)\n",
    "                input_tok = input_tok.to(device)\n",
    "                input_tok_type = input_tok_type.to(device)\n",
    "                input_tok_pos = input_tok_pos.to(device)\n",
    "                input_tok_mask = input_tok_mask.to(device)\n",
    "                input_ent_text = input_ent_text.to(device)\n",
    "                input_ent_text_length = input_ent_text_length.to(device)\n",
    "                input_ent = input_ent.to(device)\n",
    "                input_ent_type = input_ent_type.to(device)\n",
    "                input_ent_mask_type = input_ent_mask_type.to(device)\n",
    "                input_ent_mask = input_ent_mask.to(device)\n",
    "                candidate_entity_set = candidate_entity_set.to(device)\n",
    "                with torch.no_grad():\n",
    "                    tok_outputs, ent_outputs = model(input_tok, input_tok_type, input_tok_pos, input_tok_mask,\n",
    "                                    input_ent_text, input_ent_text_length, input_ent_mask_type,\n",
    "                                    input_ent, input_ent_type, input_ent_mask, candidate_entity_set)\n",
    "                if save_only_centroid:\n",
    "#                     table_mean_rep = {}\n",
    "                    new_mean = torch.mean(ent_outputs[1][0], 0, dtype=torch.float)\n",
    "                    if col not in table_representations[table_id]:\n",
    "#                         table_mean_rep['entities_only'] = new_mean\n",
    "                        table_representations[table_id][col] = new_mean\n",
    "                    else:\n",
    "                        current_mean = table_representations[table_id][col]\n",
    "                        new_mean_add = current_mean.add(new_mean)/2\n",
    "                        change = 1-cosine(current_mean, new_mean_add)\n",
    "#                         print(change)\n",
    "                        if epsilon < change or iters > 10:\n",
    "#                             print(cosine(current_mean, new_mean_add))\n",
    "                            break\n",
    "                        else:\n",
    "#                             table_mean_rep['entities_only'] = new_mean_add\n",
    "                            table_representations[table_id][col] = new_mean_add\n",
    "                    if tok_outputs in locals():\n",
    "                        del tok_outputs, ent_outputs,input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "                            input_ent, input_ent_text, input_ent_text_length, input_ent_type, input_ent_mask_type, input_ent_mask, \\\n",
    "                            candidate_entity_set\n",
    "            if col in table_representations[table_id]:\n",
    "                final_rep = table_representations[table_id][col]\n",
    "                table_representations[table_id][col] = {}\n",
    "                table_representations[table_id][col]['entities_only'] = final_rep.cpu().numpy().tolist()\n",
    "                times.append(time.time() - start_time)\n",
    "        i+=1\n",
    "        if i % 10 == 0:\n",
    "#             np.savez('mydata.npz', **table_representations)\n",
    "#             print(table_representations[table_id][col])\n",
    "            json.dump(table_representations, open(save_format.format(i-batches, i),'w'))\n",
    "#             break\n",
    "            del table_representations\n",
    "            if tok_outputs in locals():\n",
    "                del tok_outputs, ent_outputs,input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "                    input_ent, input_ent_text, input_ent_text_length, input_ent_type, input_ent_mask_type, input_ent_mask, \\\n",
    "                    candidate_entity_set\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            table_representations = {}        \n",
    "    json.dump(table_representations, open(save_format.format(i-batches, i),'w'))\n",
    "    table_representations = read_table_representations(folder)\n",
    "    return table_representations, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tables = 'data/original_ready_tables/' #folder containing folders of tables (integration sets)\n",
    "tables, table_to_cluster = read_tables(path_tables + '*')\n",
    "save_to = './FD_input/original_ready_tables/'\n",
    "\n",
    "table_representations, times = get_columns_represenation_data_lake_increment(tables, text_to_entity,save_to , 30, 'random', True, 10, 0.05)\n",
    "table_representations = read_table_representations(save_to)\n",
    "\n",
    "table_representations_by_seed = {}\n",
    "for table in table_representations:\n",
    "    seed_table = table_to_cluster[table]\n",
    "    if seed_table not in table_representations_by_seed:\n",
    "        table_representations_by_seed[seed_table] = {}\n",
    "    table_representations_by_seed[seed_table][table] = table_representations[table]\n",
    "for seed in table_representations_by_seed:\n",
    "    json.dump(table_representations_by_seed[seed], open(f'{save_to}/table_representations_'+ seed + '.json','w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
